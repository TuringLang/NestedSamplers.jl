var documenterSearchIndex = {"docs":
[{"location":"benchmarks/#Benchmarks","page":"Benchmarks","title":"Benchmarks","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"The following benchmarks show the performance of NestedSamplers.jl. As with any statistical inference package, the likelihood function will often dominate the runtime. This is important to consider when comparing packages across different languages- in general a custom Julia likelihood function may be faster than the same code written in Python/numpy. As an example, compare the relative timings of these two simple Guassian likelihoods","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"using BenchmarkTools\nusing PyCall\n\n# julia version\ngauss_loglike(X) = sum(x -> exp(-0.5 * x^2) / sqrt(2π), X)\n\n# python version\npy\"\"\"\nimport numpy as np\ndef gauss_loglike(X):\n    return np.sum(np.exp(-0.5 * X ** 2) / np.sqrt(2 * np.pi))\n\"\"\"\ngauss_loglike_py = py\"gauss_loglike\"\nxs = randn(100)","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"@btime gauss_loglike($xs)","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"  611.971 ns (0 allocations: 0 bytes)\n26.813747896467206","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"\n@btime gauss_loglike_py($xs)","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"  13.129 μs (6 allocations: 240 bytes)\n26.81374789646721","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"In certain cases, you can use language interop tools (like PyCall.jl) to use Julia likelihoods with Python libraries.","category":"page"},{"location":"benchmarks/#Setup-and-system-information","page":"Benchmarks","title":"Setup and system information","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"The benchmark code can be found in the bench folder. The system information at the time these benchmarks were ran is","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"julia> versioninfo()\nJulia Version 1.7.1\nCommit ac5cc99908* (2021-12-22 19:35 UTC)\nPlatform Info:\n  OS: macOS (x86_64-apple-darwin20.5.0)\n  CPU: Intel(R) Core(TM) i5-8259U CPU @ 2.30GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-12.0.1 (ORCJIT, skylake)\nEnvironment:\n  JULIA_NUM_THREADS = 1","category":"page"},{"location":"benchmarks/#Highly-correlated-multivariate-Guassian","page":"Benchmarks","title":"Highly-correlated multivariate Guassian","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"This benchmark uses Models.CorrelatedGaussian and simply measures the time it takes to fully sample down to dlogz=0.01. This benchmark is exactly the same as the first benchmark detailed in the JAXNS paper.","category":"page"},{"location":"benchmarks/#Timing","page":"Benchmarks","title":"Timing","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"using CSV, DataFrames, NestedSamplers, StatsPlots # hide\nbenchdir = joinpath(dirname(pathof(NestedSamplers)), \"..\", \"bench\") # hide\nresults = DataFrame(CSV.File(joinpath(benchdir, \"sampling_results.csv\"))) # hide\ngroups = groupby(results, :library) # hide\n@df groups[1] plot(:D, :t, label=\"NestedSamplers.jl\", marker=:o, yscale=:log10, # hide\n    ylabel=\"runtime (s)\", xlabel=\"prior dimension\", leg=:topleft, # hide\n    ylims=(1e-2, 1e4), markerstrokecolor=:auto) # hide\n@df groups[2] plot!(:D, :t, label=\"dynesty\", marker=:triangle, markerstrokecolor=:auto) # hide","category":"page"},{"location":"benchmarks/#Accuracy","page":"Benchmarks","title":"Accuracy","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"The following shows the Bayesian evidence estmiate as compared to the true value","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"@df groups[1] plot(:D, :dlnZ, yerr=:lnZstd, label=\"NestedSamplers.jl\", # hide\n    marker=:o, ylabel=\"ΔlnZ\", xlabel=\"prior dimension\", # hide\n    leg=:topleft, markerstrokecolor=:auto) # hide\n@df groups[2] plot!(:D, :dlnZ, yerr=:lnZstd, label=\"dynesty\", # hide\n    marker=:o, markerstrokecolor=:auto) # hide\nhline!([0.0], c=:black, ls=:dash, alpha=0.7, label=\"\") # hide","category":"page"},{"location":"api/#API/Reference","page":"API/Reference","title":"API/Reference","text":"","category":"section"},{"location":"api/","page":"API/Reference","title":"API/Reference","text":"","category":"page"},{"location":"api/#Samplers","page":"API/Reference","title":"Samplers","text":"","category":"section"},{"location":"api/","page":"API/Reference","title":"API/Reference","text":"NestedModel\nNested","category":"page"},{"location":"api/#NestedSamplers.NestedModel","page":"API/Reference","title":"NestedSamplers.NestedModel","text":"NestedModel(loglike, prior_transform)\nNestedModel(loglike, priors::AbstractVector{<:Distribution})\n\nloglike must be callable with a signature loglike(::AbstractVector) where the length of the vector must match the number of parameters in your model.\n\nprior_transform must be a callable with a signature prior_transform(::AbstractVector) that returns the transformation from the unit-cube to parameter space. This is effectively the quantile or ppf of a statistical distribution. For convenience, if a vector of Distribution is provided (as a set of priors), a transformation function will automatically be constructed using Distributions.quantile.\n\nNote: loglike is the only function used for likelihood calculations. This means if you want your priors to be used for the likelihood calculations they must be manually included in the loglike function.\n\n\n\n\n\n","category":"type"},{"location":"api/#NestedSamplers.Nested","page":"API/Reference","title":"NestedSamplers.Nested","text":"Nested(ndims, nactive;\n    bounds=Bounds.MultiEllipsoid,\n    proposal=:auto,\n    enlarge=1.25,\n    update_interval=default_update_interval(proposal, ndims),\n    min_ncall=2nactive,\n    min_eff=0.10)\n\nStatic nested sampler with nactive active points and ndims parameters.\n\nndims is equivalent to the number of parameters to fit, which defines the dimensionality of the prior volume used in evidence sampling. nactive is the number of live or active points in the prior volume. This is a static sampler, so the number of live points will be constant for all of the sampling.\n\nBounds and Proposals\n\nbounds declares the Type of Bounds.AbstractBoundingSpace to use in the prior volume. The available bounds are described by Bounds. proposal declares the algorithm used for proposing new points. The available proposals are described in Proposals. If proposal is :auto, will choose the proposal based on ndims\n\nndims < 10 - Proposals.Rejection\n10 ≤ ndims ≤ 20 - Proposals.RWalk\nndims > 20 - Proposals.Slice\n\nThe original nested sampling algorithm is roughly equivalent to using Bounds.Ellipsoid with Proposals.Rejection. The MultiNest algorithm is roughly equivalent to Bounds.MultiEllipsoid with Proposals.Rejection. The PolyChord algorithm is roughly equivalent to using Proposals.RSlice.\n\nOther Parameters\n\nenlarge - When fitting the bounds to live points, they will be enlarged (in terms of volume) by this linear factor.\nupdate_interval - How often to refit the live points with the bounds as a fraction of nactive. By default this will be determined using default_update_interval for the given proposal\nProposals.Rejection - 1.5\nProposals.RWalk and Proposals.RStagger - 0.15 * walks\nProposals.Slice - 0.9 * ndims * slices\nProposals.RSlice - 2 * slices\nmin_ncall: The minimum number of iterations before fitting the first bound; used to \n\navoid shrinking the bounds before burn-in is completed. By default 2*nactive.\n\nmin_eff: Minimum efficiency (samples accepted / samples generated) before fitting the \n\nfirst bound; used to avoid shrinking the bounds before burn-in is completed By default 0.1.\n\n\n\n\n\n","category":"type"},{"location":"api/#Convergence","page":"API/Reference","title":"Convergence","text":"","category":"section"},{"location":"api/","page":"API/Reference","title":"API/Reference","text":"There are a few convergence criteria available, by default the dlogz criterion will be used.","category":"page"},{"location":"api/","page":"API/Reference","title":"API/Reference","text":"dlogz=0.5 sample until the fraction of the remaining evidence is below the given value (more info).\nmaxiter=Inf stop after the given number of iterations\nmaxcall=Inf stop after the given number of  log-likelihood function calls\nmaxlogl=Inf stop after reaching the target log-likelihood","category":"page"},{"location":"api/#Bounds","page":"API/Reference","title":"Bounds","text":"","category":"section"},{"location":"api/","page":"API/Reference","title":"API/Reference","text":"Bounds\nBounds.AbstractBoundingSpace\nBounds.NoBounds\nBounds.Ellipsoid\nBounds.MultiEllipsoid","category":"page"},{"location":"api/#NestedSamplers.Bounds","page":"API/Reference","title":"NestedSamplers.Bounds","text":"NestedSamplers.Bounds\n\nThis module contains the different algorithms for bounding the prior volume.\n\nThe available implementations are\n\nBounds.NoBounds - no bounds on the prior volume (equivalent to a unit cube)\nBounds.Ellipsoid - bound using a single ellipsoid\nBounds.MultiEllipsoid - bound using multiple ellipsoids in an optimal cluster\n\n\n\n\n\n","category":"module"},{"location":"api/#NestedSamplers.Bounds.AbstractBoundingSpace","page":"API/Reference","title":"NestedSamplers.Bounds.AbstractBoundingSpace","text":"Bounds.AbstractBoundingSpace{T<:Number}\n\nAbstract type for describing the bounding algorithms. For information about the interface, see the extended help (??Bounds.AbstractBoundingSpace)\n\nExtended Help\n\nInterface\n\nThe following functionality defines the interface for AbstractBoundingSpace for an example type ::MyBounds\n\nFunction Required Description\nBase.rand(::AbstractRNG, ::MyBounds) x Sample a single point from the prior volume\nBounds.randoffset(::AbstractRNG, ::MyBounds)  Get a random offset from the center of the bounds. Required for random walk schemes, although a fallback is provided.\nBase.in(point, ::MyBounds) x Checks if the point is contained by the bounding space\nBounds.scale!(::MyBounds, factor) x Scale the volume by the linear factor\nBounds.volume(::MyBounds)  Retrieve the current prior volume occupied by the bounds.\nBounds.fit(::Type{<:MyBounds}, points, pointvol=0) x update the bounds given the new points each with minimum volume pointvol\nBounds.axes(::MyBounds)  Used for transforming points from the unit cube to the encompassing bound. Worth storing as a property.\n\n\n\n\n\n","category":"type"},{"location":"api/#NestedSamplers.Bounds.NoBounds","page":"API/Reference","title":"NestedSamplers.Bounds.NoBounds","text":"Bounds.NoBounds([T=Float64], N)\n\nUnbounded prior volume; equivalent to the unit cube in N dimensions. This matches the original nested sampling derivation in Skilling (2004).[1]\n\n[1]: John Skilling, 2004, AIP 735, 395  \"Nested Sampling\"\n\n\n\n\n\n","category":"type"},{"location":"api/#NestedSamplers.Bounds.Ellipsoid","page":"API/Reference","title":"NestedSamplers.Bounds.Ellipsoid","text":"Bounds.Ellipsoid([T=Float64], N)\nBounds.Ellipsoid(center::AbstractVector, A::AbstractMatrix)\n\nAn N-dimensional ellipsoid defined by\n\n(x - center)^T A (x - center) = 1\n\nwhere size(center) == (N,) and size(A) == (N,N).\n\nThis implementation follows the algorithm presented in Mukherjee et al. (2006).[2]\n\n[2]: Pia Mukherjee, et al., 2006, ApJ 638 L51 \"A Nested Sampling Algorithm for Cosmological Model Selection\"\n\n\n\n\n\n","category":"type"},{"location":"api/#NestedSamplers.Bounds.MultiEllipsoid","page":"API/Reference","title":"NestedSamplers.Bounds.MultiEllipsoid","text":"Bounds.MultiEllipsoid([T=Float64], ndims)\nBounds.MultiEllipsoid(::AbstractVector{Ellipsoid})\n\nUse multiple Ellipsoids in an optimal clustering to bound prior space. This implementation follows the MultiNest implementation outlined in Feroz et al. (2008,2009).[3][4] For more details about the bounding algorithm, see the extended help (??Bounds.MultiEllipsoid)\n\n[3]: Feroz and Hobson, 2008, MNRAS 384, 2 \"Multimodal nested sampling: an efficient and robust alternative to Markov Chain Monte Carlo methods for astronomical data analyses\"\n\n[4]: Feroz et al., 2009, MNRAS 398, 4 \"MultiNest: an efficient and robust Bayesian inference tool for cosmology and particle physics\"\n\nExtended help\n\nThe multiple-ellipsoidal implementation is defined as follows:\n\nFit a Bounds.Ellipsoid to the sample.\nPerform K-means clustering (here using Clustering.jl) centered at the endpoints of the bounding ellipsoid. This defines two clusters within the sample.\nIf either cluster has fewer than two points, consider it ill-defined and end any recursion.\nFit Bounds.Ellipsoid to each of the clusters assigned in (2).\nIf the volume of the parent ellipsoid is more than twice the volume of the two child ellipsoids, recurse (1-5) to each child.\n\nTo sample from this distribution, a random ellipsoid is selected, and a random sample is sampled from that ellipsoid. We then reverse this and find all of the ellipsoids which enclose the sampled point, and select one of those randomly for the enclosing bound.\n\n\n\n\n\n","category":"type"},{"location":"api/#Proposals","page":"API/Reference","title":"Proposals","text":"","category":"section"},{"location":"api/","page":"API/Reference","title":"API/Reference","text":"Proposals\nProposals.AbstractProposal\nProposals.Rejection\nProposals.RWalk\nProposals.RStagger\nProposals.Slice\nProposals.RSlice","category":"page"},{"location":"api/#NestedSamplers.Proposals","page":"API/Reference","title":"NestedSamplers.Proposals","text":"NestedSamplers.Proposals\n\nThis module contains the different algorithms for proposing new points within a bounding volume in unit space.\n\nThe available implementations are\n\nProposals.Rejection - samples uniformly within the bounding volume\nProposals.RWalk - random walks to a new point given an existing one\nProposals.RStagger - random staggering away to a new point given an existing one\nProposals.Slice - slicing away to a new point given an existing one\nProposals.RSlice - random slicing away to a new point given an existing one\n\n\n\n\n\n","category":"module"},{"location":"api/#NestedSamplers.Proposals.AbstractProposal","page":"API/Reference","title":"NestedSamplers.Proposals.AbstractProposal","text":"NestedSamplers.AbstractProposal\n\nThe abstract type for live point proposal algorithms.\n\nInterface\n\nEach AbstractProposal must have this function, \n\n(::AbstractProposal)(::AbstractRNG, point, loglstar, bounds, loglikelihood, prior_transform)\n\nwhich, given the input point with loglikelihood loglstar inside a bounds, returns a new point in unit space, prior space, the loglikelihood, and the number of function calls.\n\n\n\n\n\n","category":"type"},{"location":"api/#NestedSamplers.Proposals.Rejection","page":"API/Reference","title":"NestedSamplers.Proposals.Rejection","text":"Proposals.Rejection(;maxiter=100_000)\n\nPropose a new live point by uniformly sampling within the bounding volume and rejecting samples that do not meet the likelihood constraints. This follows the original nested sampling algorithm proposed in Skilling (2004)[1]\n\n[1]: John Skilling, 2004, AIP 735, 395  \"Nested Sampling\"\n\nParameters\n\nmaxiter is the maximum number of samples that can be rejected before giving up and throwing an error.\n\n\n\n\n\n","category":"type"},{"location":"api/#NestedSamplers.Proposals.RWalk","page":"API/Reference","title":"NestedSamplers.Proposals.RWalk","text":"Proposals.RWalk(;ratio=0.5, walks=25, scale=1)\n\nPropose a new live point by random walking away from an existing live point. This follows the algorithm outlined in Skilling (2006).[5]\n\n[5]: Skilling, 2006, Bayesian Anal. 1(4), \"Nested sampling for general Bayesian computation\"\n\nParameters\n\nratio is the target acceptance ratio\nwalks is the minimum number of steps to take\nscale is the proposal distribution scale, which will update between proposals.\n\n\n\n\n\n","category":"type"},{"location":"api/#NestedSamplers.Proposals.RStagger","page":"API/Reference","title":"NestedSamplers.Proposals.RStagger","text":"Proposals.RStagger(;ratio=0.5, walks=25, scale=1)\n\nPropose a new live point by random staggering away from an existing live point.  This differs from the random walk proposal in that the step size here is exponentially adjusted to reach a target acceptance rate during each proposal, in addition to between proposals. This follows the algorithm outlined in Skilling (2006).[5]\n\n[5]: Skilling, 2006, Bayesian Anal. 1(4), \"Nested sampling for general Bayesian computation\"\n\nParameters\n\nratio is the target acceptance ratio\nwalks is the minimum number of steps to take\nscale is the proposal distribution scale, which will update between proposals.\n\n\n\n\n\n","category":"type"},{"location":"api/#NestedSamplers.Proposals.Slice","page":"API/Reference","title":"NestedSamplers.Proposals.Slice","text":"Proposals.Slice(;slices=5, scale=1)\n\nPropose a new live point by a series of random slices away from an existing live point. This is a standard Gibbs-like implementation where a single multivariate slice is a combination of slices univariate slices through each axis. This follows the algorithm outlined in Neal (2003).[6]\n\n[6]: Neal, 2003, Ann. Statist. 31(3), \"Slice Sampling\"\n\nParameters\n\nslices is the minimum number of slices\nscale is the proposal distribution scale, which will update between proposals.\n\n\n\n\n\n","category":"type"},{"location":"api/#NestedSamplers.Proposals.RSlice","page":"API/Reference","title":"NestedSamplers.Proposals.RSlice","text":"Proposals.RSlice(;slices=5, scale=1)\n\nPropose a new live point by a series of random slices away from an existing live point. This is a standard random implementation where each slice is along a random direction based on the provided axes. This more closely matches the PolyChord implementation outlined in Handley et al. (2015a,b).[7][8]\n\n[7]: Handley, et al., 2015a, MNRAS 450(1), \"polychord: nested sampling for cosmology\"\n\n[8]: Handley, et al., 2015b, MNRAS 453(4), \"polychord: next-generation nested sampling\"\n\nParameters\n\nslices is the minimum number of slices\nscale is the proposal distribution scale, which will update between proposals.\n\n\n\n\n\n","category":"type"},{"location":"api/#Models","page":"API/Reference","title":"Models","text":"","category":"section"},{"location":"api/","page":"API/Reference","title":"API/Reference","text":"Models\nModels.GaussianShells\nModels.CorrelatedGaussian\nModels.Eggbox","category":"page"},{"location":"api/#NestedSamplers.Models","page":"API/Reference","title":"NestedSamplers.Models","text":"This module contains various statistical models in the form of NestedModels. These models can be used for examples and for testing.\n\nModels.GaussianShells\nModels.CorrelatedGaussian\n\n\n\n\n\n","category":"module"},{"location":"api/#NestedSamplers.Models.GaussianShells","page":"API/Reference","title":"NestedSamplers.Models.GaussianShells","text":"Models.GaussianShells()\n\n2-D Gaussian shells centered at [-3.5, 0] and [3.5, 0] with a radius of 2 and a shell width of 0.1\n\nExamples\n\njulia> model, lnZ = Models.GaussianShells();\n\njulia> lnZ\n-1.75\n\n\n\n\n\n","category":"function"},{"location":"api/#NestedSamplers.Models.CorrelatedGaussian","page":"API/Reference","title":"NestedSamplers.Models.CorrelatedGaussian","text":"Models.CorrelatedGaussian(ndims)\n\nCreates a highly-correlated Gaussian with the given dimensionality.\n\nmathbftheta sim mathcalNleft(2mathbf1 mathbfIright)\n\nSigma_ij = begincases 1 quad i=j  095 quad ineq j endcases\n\nmathcalL(mathbftheta) = mathcalNleft(mathbftheta  mathbf0 mathbfSigma right)\n\nthe analytical evidence of the model is\n\nZ = mathcalNleft(2mathbf1  mathbf0 mathbfSigma + mathbfI right)\n\nExamples\n\njulia> model, lnZ = Models.CorrelatedGaussian(10);\n\njulia> lnZ\n-12.482738597926607\n\n\n\n\n\n","category":"function"},{"location":"api/#NestedSamplers.Models.Eggbox","page":"API/Reference","title":"NestedSamplers.Models.Eggbox","text":"Models.Eggbox()\n\nEggbox/Egg carton likelihood function\n\nz(x y) = lefta + cosfracxb cdot cosfracxb right^5\n\nExamples\n\njulia> model, lnZ = Models.Eggbox();\n\njulia> lnZ\n235.88\n\n\n\n\n\n","category":"function"},{"location":"correlated/","page":"Correlated Gaussian","title":"Correlated Gaussian","text":"EditURL = \"https://github.com/TuringLang/NestedSamplers.jl/blob/main/examples/correlated.jl\"","category":"page"},{"location":"correlated/#Correlated-Gaussian","page":"Correlated Gaussian","title":"Correlated Gaussian","text":"","category":"section"},{"location":"correlated/","page":"Correlated Gaussian","title":"Correlated Gaussian","text":"This example will explore a highly-correlated Gaussian using Models.CorrelatedGaussian. This model uses a conjuage Gaussian prior, see the docstring for the mathematical definition.","category":"page"},{"location":"correlated/#Setup","page":"Correlated Gaussian","title":"Setup","text":"","category":"section"},{"location":"correlated/","page":"Correlated Gaussian","title":"Correlated Gaussian","text":"For this example, you'll need to add the following packages","category":"page"},{"location":"correlated/","page":"Correlated Gaussian","title":"Correlated Gaussian","text":"julia>]add Distributions MCMCChains Measurements NestedSamplers StatsBase StatsPlots","category":"page"},{"location":"correlated/","page":"Correlated Gaussian","title":"Correlated Gaussian","text":"using AbstractMCMC\nusing Random\nAbstractMCMC.setprogress!(false)\nRandom.seed!(8452)","category":"page"},{"location":"correlated/#Define-model","page":"Correlated Gaussian","title":"Define model","text":"","category":"section"},{"location":"correlated/","page":"Correlated Gaussian","title":"Correlated Gaussian","text":"using NestedSamplers\n\n# set up a 4-dimensional Gaussian\nD = 4\nmodel, logz = Models.CorrelatedGaussian(D);\nnothing #hide","category":"page"},{"location":"correlated/","page":"Correlated Gaussian","title":"Correlated Gaussian","text":"let's take a look at a couple of parameters to see what the likelihood surface looks like","category":"page"},{"location":"correlated/","page":"Correlated Gaussian","title":"Correlated Gaussian","text":"using StatsPlots\n\nθ1 = range(-1, 1, length=1000)\nθ2 = range(-1, 1, length=1000)\nloglike = model.prior_transform_and_loglikelihood.loglikelihood\nlogf = [loglike([t1, t2, 0, 0]) for t2 in θ2, t1 in θ1]\nheatmap(\n    θ1, θ2, exp.(logf),\n    aspect_ratio=1,\n    xlims=extrema(θ1),\n    ylims=extrema(θ2),\n    xlabel=\"θ1\",\n    ylabel=\"θ2\"\n)","category":"page"},{"location":"correlated/#Sample","page":"Correlated Gaussian","title":"Sample","text":"","category":"section"},{"location":"correlated/","page":"Correlated Gaussian","title":"Correlated Gaussian","text":"using MCMCChains\nusing StatsBase\n# using single Ellipsoid for bounds\n# using Gibbs-style slicing for proposing new points\nsampler = Nested(D, 50D;\n    bounds=Bounds.Ellipsoid,\n    proposal=Proposals.Slice()\n)\nnames = [\"θ_$i\" for i in 1:D]\nchain, state = sample(model, sampler; dlogz=0.01, param_names=names)\n# resample chain using statistical weights\nchain_resampled = sample(chain, Weights(vec(chain[:weights])), length(chain));\nnothing #hide","category":"page"},{"location":"correlated/#Results","page":"Correlated Gaussian","title":"Results","text":"","category":"section"},{"location":"correlated/","page":"Correlated Gaussian","title":"Correlated Gaussian","text":"chain_resampled","category":"page"},{"location":"correlated/","page":"Correlated Gaussian","title":"Correlated Gaussian","text":"corner(chain_resampled)","category":"page"},{"location":"correlated/","page":"Correlated Gaussian","title":"Correlated Gaussian","text":"using Measurements\nlogz_est = state.logz ± state.logzerr\ndiff = logz_est - logz\nprintln(\"logz: $logz\")\nprintln(\"estimate: $logz_est\")\nprintln(\"diff: $diff\")","category":"page"},{"location":"correlated/","page":"Correlated Gaussian","title":"Correlated Gaussian","text":"","category":"page"},{"location":"correlated/","page":"Correlated Gaussian","title":"Correlated Gaussian","text":"This page was generated using Literate.jl.","category":"page"},{"location":"eggbox/","page":"Eggbox","title":"Eggbox","text":"EditURL = \"https://github.com/TuringLang/NestedSamplers.jl/blob/main/examples/eggbox.jl\"","category":"page"},{"location":"eggbox/#Eggbox","page":"Eggbox","title":"Eggbox","text":"","category":"section"},{"location":"eggbox/","page":"Eggbox","title":"Eggbox","text":"This example will explore the classic eggbox function using Models.Eggbox.","category":"page"},{"location":"eggbox/#Setup","page":"Eggbox","title":"Setup","text":"","category":"section"},{"location":"eggbox/","page":"Eggbox","title":"Eggbox","text":"For this example, you'll need to add the following packages","category":"page"},{"location":"eggbox/","page":"Eggbox","title":"Eggbox","text":"julia>]add Distributions MCMCChains Measurements NestedSamplers StatsBase StatsPlots","category":"page"},{"location":"eggbox/","page":"Eggbox","title":"Eggbox","text":"using AbstractMCMC\nusing Random\nAbstractMCMC.setprogress!(false)\nRandom.seed!(8452)","category":"page"},{"location":"eggbox/#Define-model","page":"Eggbox","title":"Define model","text":"","category":"section"},{"location":"eggbox/","page":"Eggbox","title":"Eggbox","text":"using NestedSamplers\n\nmodel, logz = Models.Eggbox();\nnothing #hide","category":"page"},{"location":"eggbox/","page":"Eggbox","title":"Eggbox","text":"let's take a look at a couple of parameters to see what the log-likelihood surface looks like","category":"page"},{"location":"eggbox/","page":"Eggbox","title":"Eggbox","text":"using StatsPlots\n\nx = range(0, 1, length=1000)\ny = range(0, 1, length=1000)\nloglike = model.prior_transform_and_loglikelihood.loglikelihood\nlogf = [loglike([xi, yi]) for yi in y, xi in x]\nheatmap(\n    x, y, logf,\n    xlims=extrema(x),\n    ylims=extrema(y),\n    xlabel=\"x\",\n    ylabel=\"y\",\n)","category":"page"},{"location":"eggbox/#Sample","page":"Eggbox","title":"Sample","text":"","category":"section"},{"location":"eggbox/","page":"Eggbox","title":"Eggbox","text":"using MCMCChains\nusing StatsBase\n# using multi-ellipsoid for bounds\n# using default rejection sampler for proposals\nsampler = Nested(2, 500)\nchain, state = sample(model, sampler; dlogz=0.01, param_names=[\"x\", \"y\"])\n# resample chain using statistical weights\nchain_resampled = sample(chain, Weights(vec(chain[:weights])), length(chain));\nnothing #hide","category":"page"},{"location":"eggbox/#Results","page":"Eggbox","title":"Results","text":"","category":"section"},{"location":"eggbox/","page":"Eggbox","title":"Eggbox","text":"chain_resampled","category":"page"},{"location":"eggbox/","page":"Eggbox","title":"Eggbox","text":"marginalkde(chain[:x], chain[:y])\nplot!(xlims=(0, 1), ylims=(0, 1), sp=2)\nplot!(xlims=(0, 1), sp=1)\nplot!(ylims=(0, 1), sp=3)","category":"page"},{"location":"eggbox/","page":"Eggbox","title":"Eggbox","text":"density(chain_resampled, xlims=(0, 1))\nvline!(0.1:0.2:0.9, c=:black, ls=:dash, sp=1)\nvline!(0.1:0.2:0.9, c=:black, ls=:dash, sp=2)","category":"page"},{"location":"eggbox/","page":"Eggbox","title":"Eggbox","text":"using Measurements\nlogz_est = state.logz ± state.logzerr\ndiff = logz_est - logz\nprintln(\"logz: $logz\")\nprintln(\"estimate: $logz_est\")\nprintln(\"diff: $diff\")","category":"page"},{"location":"eggbox/","page":"Eggbox","title":"Eggbox","text":"","category":"page"},{"location":"eggbox/","page":"Eggbox","title":"Eggbox","text":"This page was generated using Literate.jl.","category":"page"},{"location":"intro/#Introduction","page":"Introduction","title":"Introduction","text":"","category":"section"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"Nested sampling is a statistical technique first described in Skilling (2004)[1] as a method for estimating the Bayesian evidence. Conveniently, it also produces samples with importance weighting proportional to the posterior distribution. To understand what this means, we need to comprehend Bayes' theorem.","category":"page"},{"location":"intro/#Bayes'-theorem","page":"Introduction","title":"Bayes' theorem","text":"","category":"section"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"Bayes' theorem, in our nomenclature, is described as the relationship between the prior, the likelihood, the evidence, and the posterior. In its entirety-","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"p(theta  x) = fracp(x  theta)p(theta)p(x)","category":"page"},{"location":"intro/#Posterior","page":"Introduction","title":"Posterior","text":"","category":"section"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"p(theta  x) - the probability of the model parameters (theta) conditioned on the data (x)","category":"page"},{"location":"intro/#Likelihood","page":"Introduction","title":"Likelihood","text":"","category":"section"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"p(x  theta) - the probability of the data (x) conditioned on the model parameters (theta)","category":"page"},{"location":"intro/#Prior","page":"Introduction","title":"Prior","text":"","category":"section"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"p(theta) - the probability of the model parameters","category":"page"},{"location":"intro/#Evidence","page":"Introduction","title":"Evidence","text":"","category":"section"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"p(x) - the probability of the data","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"If you are familiar with Bayesian statistics and Markov Chain Monte Carlo (MCMC) techniques, you should be somewhat familiar with the relationships between the posterior, the likelihood, and the prior. The evidence, though, is somewhat hard to describe; what does \"the probability of the data\" mean? Well, another way of writing the evidence, is this integral","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"p(x)  equiv Z = int_Omegap(x  theta) mathrmdtheta","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"which is like saying \"the likelihood of the data [p(x  theta)] integrated over all of parameter space [Omega]\". We have to write the probability this way, because the data are statistically dependent on the model parameters. This integral is intractable for all but the simplest combinations of distributions (conjugate distributions), and therefore it must be estimated or approximated in some way.","category":"page"},{"location":"intro/#What-can-we-do-with-the-evidence?","page":"Introduction","title":"What can we do with the evidence?","text":"","category":"section"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"Before we get into approximating the Bayesian evidence, let's talk about why it's important. After all, for most MCMC applications it is simply a normalization factor to be ignored (how convenient!).","category":"page"},{"location":"intro/#Further-reading","page":"Introduction","title":"Further reading","text":"","category":"section"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"For further reading, I recommend reading the cited sources in the footnotes, as well as the references below","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"dynesty documentation","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"[1]: Skilling 2004","category":"page"},{"location":"shells/","page":"Gaussian Shells","title":"Gaussian Shells","text":"EditURL = \"https://github.com/TuringLang/NestedSamplers.jl/blob/main/examples/shells.jl\"","category":"page"},{"location":"shells/#Gaussian-Shells","page":"Gaussian Shells","title":"Gaussian Shells","text":"","category":"section"},{"location":"shells/","page":"Gaussian Shells","title":"Gaussian Shells","text":"This example will explore the classic Gaussian shells model using Models.GaussianShells.","category":"page"},{"location":"shells/#Setup","page":"Gaussian Shells","title":"Setup","text":"","category":"section"},{"location":"shells/","page":"Gaussian Shells","title":"Gaussian Shells","text":"For this example, you'll need to add the following packages","category":"page"},{"location":"shells/","page":"Gaussian Shells","title":"Gaussian Shells","text":"julia>]add Distributions MCMCChains Measurements NestedSamplers StatsBase StatsPlots","category":"page"},{"location":"shells/","page":"Gaussian Shells","title":"Gaussian Shells","text":"using AbstractMCMC\nusing Random\nAbstractMCMC.setprogress!(false)\nRandom.seed!(8452)","category":"page"},{"location":"shells/#Define-model","page":"Gaussian Shells","title":"Define model","text":"","category":"section"},{"location":"shells/","page":"Gaussian Shells","title":"Gaussian Shells","text":"using NestedSamplers\n\nmodel, logz = Models.GaussianShells();\nnothing #hide","category":"page"},{"location":"shells/","page":"Gaussian Shells","title":"Gaussian Shells","text":"let's take a look at a couple of parameters to see what the likelihood surface looks like","category":"page"},{"location":"shells/","page":"Gaussian Shells","title":"Gaussian Shells","text":"using StatsPlots\n\nx = range(-6, 6, length=1000)\ny = range(-2.5, 2.5, length=1000)\nloglike = model.prior_transform_and_loglikelihood.loglikelihood\nlogf = [loglike([xi, yi]) for yi in y, xi in x]\nheatmap(\n    x, y, exp.(logf),\n    xlims=extrema(x),\n    ylims=extrema(y),\n    xlabel=\"x\",\n    ylabel=\"y\",\n)","category":"page"},{"location":"shells/#Sample","page":"Gaussian Shells","title":"Sample","text":"","category":"section"},{"location":"shells/","page":"Gaussian Shells","title":"Gaussian Shells","text":"using MCMCChains\nusing StatsBase\n# using multi-ellipsoid for bounds\n# using default rejection sampler for proposals\nsampler = Nested(2, 1000)\nchain, state = sample(model, sampler; dlogz=0.05, param_names=[\"x\", \"y\"])\n# resample chain using statistical weights\nchain_resampled = sample(chain, Weights(vec(chain[:weights])), length(chain));\nnothing #hide","category":"page"},{"location":"shells/#Results","page":"Gaussian Shells","title":"Results","text":"","category":"section"},{"location":"shells/","page":"Gaussian Shells","title":"Gaussian Shells","text":"chain_resampled","category":"page"},{"location":"shells/","page":"Gaussian Shells","title":"Gaussian Shells","text":"marginalkde(chain[:x], chain[:y])\nplot!(xlims=(-6, 6), ylims=(-2.5, 2.5), sp=2)\nplot!(xlims=(-6, 6), sp=1)\nplot!(ylims=(-2.5, 2.5), sp=3)","category":"page"},{"location":"shells/","page":"Gaussian Shells","title":"Gaussian Shells","text":"density(chain_resampled)\nvline!([-5.5, -1.5, 1.5, 5.5], c=:black, ls=:dash, sp=1)\nvline!([-2, 2], c=:black, ls=:dash, sp=2)","category":"page"},{"location":"shells/","page":"Gaussian Shells","title":"Gaussian Shells","text":"using Measurements\nlogz_est = state.logz ± state.logzerr\ndiff = logz_est - logz\nprintln(\"logz: $logz\")\nprintln(\"estimate: $logz_est\")\nprintln(\"diff: $diff\")","category":"page"},{"location":"shells/","page":"Gaussian Shells","title":"Gaussian Shells","text":"","category":"page"},{"location":"shells/","page":"Gaussian Shells","title":"Gaussian Shells","text":"This page was generated using Literate.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = NestedSamplers","category":"page"},{"location":"#NestedSamplers.jl","page":"Home","title":"NestedSamplers.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(Image: GitHub) (Image: Build Status) (Image: PkgEval) (Image: Coverage) (Image: LICENSE)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Implementations of single- and multi-ellipsoidal nested sampling algorithms in pure Julia. We implement the AbstractMCMC.jl interface, allowing straightforward sampling from a variety of statistical models.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package was heavily influenced by nestle, dynesty, and NestedSampling.jl.","category":"page"},{"location":"#Citing","page":"Home","title":"Citing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(Image: DOI)","category":"page"},{"location":"","page":"Home","title":"Home","text":"If you use this library, or a derivative of it, in your work, please consider citing it. This code is built off a multitude of academic works, which have been noted in the docstrings where appropriate. These references, along with references for the more general calculations, can all be found in CITATION.bib","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To use the nested samplers first install this library","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ]add NestedSamplers","category":"page"},{"location":"#Background","page":"Home","title":"Background","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"For statistical background and a more in-depth introduction to nested sampling, I recommend the dynesty documentation. In short, nested sampling is a technique for simultaneously estimating the Bayesian evidence and the posterior distribution (according to Bayes' theorem) from nested iso-likelihood shells. These shells allow a quadrature estimate of the integral for the Bayesian evidence, which we can use for model selection, as well as the statistical weights for the underlying \"live\" points, which is where we get our posterior samples from!","category":"page"},{"location":"#Usage","page":"Home","title":"Usage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The samplers are built using the AbstractMCMC.jl interface. To use it, we need to create a NestedModel.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Random\nusing AbstractMCMC\nAbstractMCMC.setprogress!(false)\nRandom.seed!(8452);\nnothing # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Distributions\nusing LinearAlgebra\nusing NestedSamplers\nusing LogExpFunctions: logaddexp\n\n# Gaussian mixture model\nσ = 0.1\nμ1 = ones(2)\nμ2 = -ones(2)\ninv_σ = diagm(0 => fill(1 / σ^2, 2))\n\nfunction logl(x)\n    dx1 = x .- μ1\n    dx2 = x .- μ2\n    f1 = -dx1' * (inv_σ * dx1) / 2\n    f2 = -dx2' * (inv_σ * dx2) / 2\n    return logaddexp(f1, f2)\nend\npriors = [\n    Uniform(-5, 5),\n    Uniform(-5, 5)\n]\n# or equivalently\nprior_transform(X) = 10 .* X .- 5\n# create the model\n# or model = NestedModel(logl, prior_transform)\nmodel = NestedModel(logl, priors);\nnothing # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"now, we set up our sampling using StatsBase.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Important:  the state of the sampler is returned in addition to the chain by sample.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using StatsBase: sample, Weights\n\n# create our sampler\n# 2 parameters, 1000 active points, multi-ellipsoid. See docstring\nspl = Nested(2, 1000)\n# by default, uses dlogz for convergence. Set the keyword args here\n# currently Chains and Array are support chain_types\nchain, state = sample(model, spl; dlogz=0.2, param_names=[\"x\", \"y\"])\n# optionally resample the chain using the weights\nchain_res = sample(chain, Weights(vec(chain[\"weights\"])), length(chain));","category":"page"},{"location":"","page":"Home","title":"Home","text":"let's take a look at the resampled posteriors","category":"page"},{"location":"","page":"Home","title":"Home","text":"using StatsPlots\ndensity(chain_res)\n# analytical posterior maxima\nvline!([-1, 1], c=:black, ls=:dash, subplot=1)\nvline!([-1, 1], c=:black, ls=:dash, subplot=2)","category":"page"},{"location":"","page":"Home","title":"Home","text":"and compare our estimate of the Bayesian (log-)evidence to the analytical value","category":"page"},{"location":"","page":"Home","title":"Home","text":"analytic_logz = log(4π * σ^2 / 100)\n# within 2-sigma\n@assert isapprox(analytic_logz, state.logz, atol=2state.logzerr)","category":"page"},{"location":"#Contributions-and-Support","page":"Home","title":"Contributions and Support","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(Image: ColPrac: Contributor's Guide on Collaborative Practices for Community Packages)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Primary Author: Miles Lucas (@mileslucas)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Contributions are always welcome! In general, contributions should follow ColPrac. Take a look at the issues for ideas of open problems! To discuss ideas or plan contributions, open a discussion.","category":"page"}]
}
